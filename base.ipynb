{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/prnv007-rgb/CareerKraft\",\n",
    "    repo_path=\"./CareerKraft\",\n",
    "    branch=\"main\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\")  # optional\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_repo_texts(repo_path):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.py', '.js', '.ts', '.md', '.html', '.json')):  # filter as needed\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "repo_texts = load_repo_texts('./CareerKraft')\n",
    "\n",
    "# Then chunk, embed, and store in FAISS or your RAG vector DB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5eaea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Loaded 3 source files from https://github.com/prnv007-rgb/Book-Recommendations-.git\n",
      "‚úÇÔ∏è  Split into 2 chunks (size=800, overlap=100)\n",
      "‚úÖ FAISS index saved to: ./faiss_index/careerkraft\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings   # or replace with your favorite\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def build_rag_index(\n",
    "    repo_url: str,\n",
    "    local_path: str = \"./repos\",\n",
    "    branch: str = \"main\",\n",
    "    index_path: str = \"./faiss_index\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "):\n",
    "    # 1Ô∏è‚É£ Clone & load all files as Documents (adds file_path metadata)\n",
    "    loader = GitLoader(\n",
    "        clone_url=repo_url,\n",
    "        repo_path=os.path.join(local_path, Path(repo_url).stem),\n",
    "        branch=branch,\n",
    "        # optional filter: only code files\n",
    "        file_filter=lambda f: f.endswith((\".py\", \".js\", \".ts\", \".java\", \".go\", \".html\", \".css\")),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"üóÇÔ∏è  Loaded {len(docs)} source files from {repo_url}\")\n",
    "\n",
    "    # 2Ô∏è‚É£ Chunk long files for better retrieval granularity\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÇÔ∏è  Split into {len(chunks)} chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Create embeddings\n",
    "    embedder = OllamaEmbeddings(model=\"mxbai-embed-large\")  \n",
    "    # ‚Äî or replace with: \n",
    "    # from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    # embedder = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "    # 4Ô∏è‚É£ Build FAISS index (automatically embeds and stores metadata)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embedder)\n",
    "    vectorstore.save_local(index_path)\n",
    "    print(f\"‚úÖ FAISS index saved to: {index_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_rag_index(\n",
    "        repo_url=\"https://github.com/prnv007-rgb/Book-Recommendations-.git\",\n",
    "        local_path=\"./repos\",\n",
    "        branch=\"main\",\n",
    "        index_path=\"./faiss_index/careerkraft\",\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49e883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedder = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./faiss_index/careerkraft\",\n",
    "    embeddings=embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50b15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fa80b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_3700\\2708833247.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "question = \"How are books recommended in this project?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5edbf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: According to the code, books are recommended based on a collaborative filtering approach using the `model.kneighbors` method, which takes into account the ratings and preferences of users who have similar tastes.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. The user selects a book as input.\n",
      "2. The model uses this book's characteristics (e.g., genre, author, etc.) to find similar books by computing the k-nearest neighbors using `model.kneighbors`.\n",
      "3. The function `rec_book` takes the selected book and returns a list of recommended books and their corresponding poster URLs.\n",
      "\n",
      "The recommendation is based on the idea that if a user liked a particular book, they may also like other books with similar characteristics. The model uses this principle to generate recommendations by finding the most similar books to the input book.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ask_llm_with_context(question, context):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant with knowledge of the following GitHub repo.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Based on the context above, answer this question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "answer = ask_llm_with_context(question, context)\n",
    "print(\"ü§ñ Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
