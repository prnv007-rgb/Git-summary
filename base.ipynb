{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab09ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/rudrajikadra/Movie-Recommendation-System-Using-Python-and-Pandas.git\",\n",
    "    repo_path=\"./Movie\",\n",
    "    branch=\"master\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\")  # optional\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1e9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_repo_texts(repo_path):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.py', '.js', '.ts', '.md', '.html', '.json')):  # filter as needed\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "repo_texts = load_repo_texts('./Movie')\n",
    "\n",
    "# Then chunk, embed, and store in FAISS or your RAG vector DB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5eaea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Loaded 6 source files from https://github.com/rudrajikadra/Movie-Recommendation-System-Using-Python-and-Pandas.git\n",
      "‚úÇÔ∏è  Split into 6 chunks (size=800, overlap=100)\n",
      "‚úÖ FAISS index saved to: ./faiss_index/movies\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings   # or replace with your favorite\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def build_rag_index(\n",
    "    repo_url: str,\n",
    "    local_path: str = \"./repos\",\n",
    "    branch: str = \"master\",\n",
    "    index_path: str = \"./faiss_index\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "):\n",
    "    # 1Ô∏è‚É£ Clone & load all files as Documents (adds file_path metadata)\n",
    "    loader = GitLoader(\n",
    "    clone_url=repo_url,\n",
    "    repo_path=os.path.join(local_path, Path(repo_url).stem),\n",
    "    branch=branch,\n",
    "    file_filter=lambda f: True  # ‚Üê TEMP: load all files\n",
    ")\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(f\"üóÇÔ∏è  Loaded {len(docs)} source files from {repo_url}\")\n",
    "\n",
    "    # 2Ô∏è‚É£ Chunk long files for better retrieval granularity\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÇÔ∏è  Split into {len(chunks)} chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Create embeddings\n",
    "    embedder = OllamaEmbeddings(model=\"mxbai-embed-large\")  \n",
    "    # ‚Äî or replace with: \n",
    "    # from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    # embedder = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "    # 4Ô∏è‚É£ Build FAISS index (automatically embeds and stores metadata)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embedder)\n",
    "    vectorstore.save_local(index_path)\n",
    "    print(f\"‚úÖ FAISS index saved to: {index_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_rag_index(\n",
    "        repo_url=\"https://github.com/rudrajikadra/Movie-Recommendation-System-Using-Python-and-Pandas.git\",\n",
    "        local_path=\"./repos\",\n",
    "        branch=\"master\",\n",
    "        index_path=\"./faiss_index/movies\",\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49e883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedder = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./faiss_index/movies\",\n",
    "    embeddings=embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50b15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa80b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_16680\\2092115758.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "question = \"How are books recommended in this project?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5edbf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: I'm happy to help! Based on the provided GitHub repo context, I can tell you that it appears to be a movie recommendation system.\n",
      "\n",
      "The system seems to contain a collection of movies with their respective ratings and other relevant information. The data is likely in a format such as CSV or JSON, which allows for easy manipulation and querying.\n",
      "\n",
      "To answer your question, here's how the system works:\n",
      "\n",
      "1. **Data Collection**: A large dataset of movies, including their titles, genres, ratings, and other relevant information, was collected.\n",
      "2. **Data Preprocessing**: The data was preprocessed to ensure consistency and remove any errors or duplicate entries.\n",
      "3. **Recommendation Algorithm**: An algorithm was developed to analyze the user's preferences and generate personalized movie recommendations based on their viewing history and ratings.\n",
      "4. **User Interaction**: Users interact with the system by searching for movies, browsing through categories, or getting personalized recommendations based on their profile.\n",
      "\n",
      "The recommendation algorithm likely uses a combination of techniques such as:\n",
      "\n",
      "1. **Collaborative Filtering**: Analyzing user behavior and preferences to recommend movies that similar users have liked or rated highly.\n",
      "2. **Content-Based Filtering**: Considering the attributes of each movie (e.g., genre, director, actor) to suggest movies that match the user's interests.\n",
      "3. **Hybrid Approach**: Combining multiple techniques above to generate more accurate recommendations.\n",
      "\n",
      "Overall, this system provides a way for users to discover new movies based on their preferences and viewing history, making it easier to find something they'll enjoy.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def ask_llm_with_context(question, context):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant with knowledge of the following GitHub repo.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Based on the context above, answer this question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "answer = ask_llm_with_context(question, context)\n",
    "print(\"ü§ñ Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
